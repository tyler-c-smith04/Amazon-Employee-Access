count(`room type`)
abnb %>%
count(`neighbourhood group`)
# Omit the misspelled 'brookln' neighbourhood group
abnb <- abnb[abnb$`neighbourhood group` != 'brookln', ]
abnb %>%
count(`neighbourhood group`)
# Only view the verified host id's
abnb <- abnb %>%
filter(host_identity_verified == 'verified')
glimpse(abnb)
abnb$price<-gsub("$"," ",as.character(abnb$price))
abnb$price<-gsub(","," ",as.character(abnb$price))
abnb
View(abnb)
abnb$price<-gsub("$"," ",as.character(abnb$price))
abnb$price<-gsub(","," ",as.character(abnb$price))
View(abnb)
abnb$price<-gsub("$"," ",as.character(abnb$price))
abnb$price<-gsub(","," ",as.character(abnb$price))
abnb
abnb$price<-gsub("$"," ",as.character(abnb$price))
# abnb$price<-gsub(","," ",as.character(abnb$price))
abnb
library(tidyverse)
library(readr)
library(stringr)
abnb <- read_csv("Development/Projects/doTERRA Assessment/Airbnb_Open_Data.csv")
abnb <- as_tibble(abnb)
abnb <- abnb %>%
select(NAME, `host id`, host_identity_verified, `neighbourhood group`,
neighbourhood, `room type`, `Construction year`, price, `service fee`)
abnb <- na.omit(abnb)
table(abnb['room type'])
table(abnb['neighbourhood group'])
abnb %>%
count(`room type`)
abnb %>%
count(`neighbourhood group`)
# Omit the misspelled 'brookln' neighbourhood group
abnb <- abnb[abnb$`neighbourhood group` != 'brookln', ]
abnb %>%
count(`neighbourhood group`)
# Only view the verified host id's
abnb <- abnb %>%
filter(host_identity_verified == 'verified')
glimpse(abnb)
abnb$price<-gsub("$"," ",as.character(abnb$price))
# abnb$price<-gsub(","," ",as.character(abnb$price))
abnb
glimpse(abnb)
abnb$price<-gsub("$"," ",as.character(abnb$price))
abnb
View(abnb)
abnb$price<-gsub("$"," ",as.character(abnb$price))
abnb
View(abnb)
abnb$price <- gsub("$"," ",as.character(abnb$price))
abnb
abnb$price <- gsub("$"," ",as.character(abnb$price))
abnb
View(abnb)
abnb
abnb$price <- gsub("$"," ",as.character(abnb$price))
abnb$price
abnb$price <- gsub("$","",as.character(abnb$price))
abnb$price
abnb$price <- gsub("$"," ",as.character(abnb$price))
abnb$price
library(tidyverse)
library(readr)
library(stringr)
abnb <- read_csv("Development/Projects/doTERRA Assessment/Airbnb_Open_Data.csv")
abnb <- as_tibble(abnb)
gsub("$", " ", as.character(abnb$price))
as.numeric(abnb$price)
abnb
abnb <- abnb %>%
select(NAME, `host id`, host_identity_verified, `neighbourhood group`,
neighbourhood, `room type`, `Construction year`, price, `service fee`)
abnb <- na.omit(abnb)
abnb
gsub("$", " ", as.character(abnb$price))
gsub("$", " ", abnb$price)
gsub("$", " ", as.numeric(abnb$price))
gsub("$", " ", abnb$price)
gsub(",", " ", abnb$price)
gsub(",", "", abnb$price)
abnb$price <- gsub(",", "", abnb$price)
abnb
gsub('$', '',abnb$price)
gsub("$"," ", abnb$price)
abnb
gsub("$", " ", abnb$price)
gsub('[^[:alnum:] ]','',abnb$price)
abnb$price <- gsub('[^[:alnum:] ]','',abnb$price)
abnb
as.numeric(abnb$price)
abnb$price <- as.numeric(abnb$price)
glimpse(abnb)
abnb$`service fee` <- gsub('[^[:alnum:] ]','',abnb$`service fee`)
abnb$`service fee` <- as.numeric(abnb$`service fee`)
glimpse(abnb)
library(tidyverse)
library(readr)
library(stringr)
abnb <- read_csv("Development/Projects/doTERRA Assessment/Airbnb_Open_Data.csv")
abnb <- as_tibble(abnb)
abnb <- abnb %>%
select(NAME, `host id`, host_identity_verified, `neighbourhood group`,
neighbourhood, `room type`, `Construction year`, price, `service fee`)
abnb <- na.omit(abnb)
table(abnb['room type'])
table(abnb['neighbourhood group'])
abnb %>%
count(`room type`)
abnb %>%
count(`neighbourhood group`)
# Omit the misspelled 'brookln' neighbourhood group
abnb <- abnb[abnb$`neighbourhood group` != 'brookln', ]
abnb %>%
count(`neighbourhood group`)
# Only view the verified host id's
abnb <- abnb %>%
filter(host_identity_verified == 'verified')
# Remove the $ and , from price and service fee, then convert to numeric
abnb$price <- gsub('[^[:alnum:] ]','',abnb$price)
abnb$price <- as.numeric(abnb$price)
abnb$`service fee` <- gsub('[^[:alnum:] ]','',abnb$`service fee`)
abnb$`service fee` <- as.numeric(abnb$`service fee`)
glimpse(abnb)
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(avg_price = mean(price))
abnb %>%
count(`neighbourhood group`)
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(avg_price = mean(price))
abnb %>%
group_by(`neighbourhood group`) %>%
aggregate(price)
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price))
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price),
mean_price = mean(pricex))
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price),
mean_price = mean(price))
abnb
abnb %>%
filter(`neighbourhood group` == 'Bronx')
bronx <- abnb %>%
filter(`neighbourhood group` == 'Bronx')
mean(bronx$price)
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price),
mean_price = mean(round(price)))
mean(bronx$price)
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price),
mean_price = round(mean(price)),2)
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price),
mean_price = round(mean(price),2))
abnb %>%
group_by(`neighbourhood group`) %>%
summarize(max_price = max(price),
mean_price = mean(price))
library(tidyverse)
library(rvest)
url <- https://www.espn.com/nba/player/stats/_/id/3908845/john-collins
url <- 'https://www.espn.com/nba/player/stats/_/id/3908845/john-collins'
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
pen_preds <- predict(preg_wf, new_data = test) %>% #This predicts log(count)
mutate(.pred=exp(.pred)) %>% # Back-transform the log to original scale
bind_cols(., test) %>% #Bind predictions with test data
select(datetime, .pred) %>% #Just keep datetime and predictions
rename(count=.pred) %>% #rename pred to count (for submission to Kaggle)
mutate(count=pmax(0, count)) %>% #pointwise max of (0, prediction)
mutate(datetime=as.character(format(datetime))) #needed for right format to Kaggle
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
colnames(predictions) <- c('datetime', 'count')
# Change formatting of datetime
predictions$datetime <- as.character(predictions$datetime)
# Write that dataset to a csv file
vroom_write(predictions, 'predictions.csv', ",")
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
## Bike Share Clean Code
library(tidyverse)
library(vroom)
library(patchwork)
library(tidymodels)
library(poissonreg)
bike <- vroom("./train.csv")
bike <- bike %>%
select(-casual, -registered)
install.packages('embed')
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed)
train <- vroom("./train.csv")
train <- vroom("./train.csv")
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
train <- vroom("./train.csv")
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%
step_dummy(all_nominal_predictors()) # dummy variable encoding
setwd("~/Desktop/STAT348/Amazon-Employee-Access")
train <- vroom("./train.csv")
test <- vroom("./test.csv")
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%
step_dummy(all_nominal_predictors()) # dummy variable encoding
# NOTE: some of these step functions are not appropriate to use together 13
# apply the recipe to your data
prepped_recipe <- prep(my_recipe)
baked <- bake(prepped_recipe, new_data = NULL)
baked
# Logistic Regression -----------------------------------------------------
library(tidymodels)
log_mod <- logistic_reg() %>% # Type of model
set_engine('glm')
amazon_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(log_mod) %>%
fit(data = train) # Fit the workflow
baked
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_mutate(ACTION = as.factor(ACTION)) %>%
step_other(all_nominal_predictors(), threshold = .01) %>%
step_dummy(all_nominal_predictors()) # dummy variable encoding
# NOTE: some of these step functions are not appropriate to use together 13
# apply the recipe to your data
prepped_recipe <- prep(my_recipe)
baked <- bake(prepped_recipe, new_data = NULL)
# Logistic Regression -----------------------------------------------------
library(tidymodels)
log_mod <- logistic_reg() %>% # Type of model
set_engine('glm')
amazon_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(log_mod) %>%
fit(data = train) # Fit the workflow
amazon_predictions <- predict(amazon_wf,
new_data = test,
type = 'class')
test <- vroom("./test.csv") %>%
select(-1)
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_mutate(ACTION = as.factor(ACTION)) %>%
step_other(all_nominal_predictors(), threshold = .01) %>%
step_dummy(all_nominal_predictors()) # dummy variable encoding
# NOTE: some of these step functions are not appropriate to use together 13
# apply the recipe to your data
prepped_recipe <- prep(my_recipe)
baked <- bake(prepped_recipe, new_data = NULL)
# Logistic Regression -----------------------------------------------------
library(tidymodels)
log_mod <- logistic_reg() %>% # Type of model
set_engine('glm')
amazon_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(log_mod) %>%
fit(data = train) # Fit the workflow
amazon_predictions <- predict(amazon_wf,
new_data = test,
type = 'class')
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01) %>%
step_dummy(all_nominal_predictors()) # dummy variable encoding
# NOTE: some of these step functions are not appropriate to use together 13
# apply the recipe to your data
prepped_recipe <- prep(my_recipe)
baked <- bake(prepped_recipe, new_data = NULL)
# Logistic Regression -----------------------------------------------------
library(tidymodels)
log_mod <- logistic_reg() %>% # Type of model
set_engine('glm')
amazon_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(log_mod) %>%
fit(data = train) # Fit the workflow
amazon_predictions <- predict(amazon_wf,
new_data = test,
type = 'class')
amazon_predictions
amazon_submission <- amazon_predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_class") %>%
select(2,1)
vroom_write(x=amazon_submission, file="./logistic_reg.csv", delim=",")
# Logistic Regression -----------------------------------------------------
library(tidymodels)
log_mod <- logistic_reg() %>% # Type of model
set_engine('glm')
amazon_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(log_mod) %>%
fit(data = train) # Fit the workflow
amazon_predictions <- predict(amazon_wf,
new_data = test,
type = 'prob')
amazon_submission <- amazon_predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_class") %>%
select(2,1)
amazon_predictions
amazon_submission <- amazon_predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_class") %>%
select(2,1)
amazon_submission <- amazon_predictions %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
vroom_write(x=amazon_submission, file="./logistic_reg.csv", delim=",")
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
setwd("~/Desktop/STAT348/Amazon-Employee-Access")
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
my_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%
step_dummy(all_nominal_predictors()) # dummy variable encoding
# Logistic Regression -----------------------------------------------------
library(tidymodels)
pen_log_mod <- logistic_reg(mixture = , penalty = ) %>%
set_engine('glmnet')
pen_log_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pen_log_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = L)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Splits data for CV
folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
cv_results <- pen_log_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc, f_meas, sens, recall, spec,
precision, accuracy))
target_encoding_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) # dummy variable encoding
pen_log_mod <- logistic_reg(mixture = , penalty = ) %>%
set_engine('glmnet')
pen_log_wf <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(pen_log_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Splits data for CV
folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
cv_results <- pen_log_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc, f_meas, sens, recall, spec,
precision, accuracy))
## Run the CV
cv_results <- pen_log_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
pen_log_wf <- workflow() %>%
add_recipe(target_encoding_recipe) %>%
add_model(pen_log_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Splits data for CV
folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
cv_results <- pen_log_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed) # for target encoding
train <- vroom("./train.csv") %>%
mutate(ACTION = as.factor(ACTION))
test <- vroom("./test.csv") %>%
select(-1)
target_encoding_recipe <- recipe(ACTION ~ ., data = train) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .001) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) # dummy variable encoding
pen_log_mod <- logistic_reg(mixture = , penalty = ) %>%
set_engine('glmnet')
pen_log_wf <- workflow() %>%
add_recipe(target_encoding_recipe) %>%
add_model(pen_log_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Splits data for CV
folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
cv_results <- pen_log_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
pen_log_mod <- logistic_reg(mixture = tune() , penalty = tune() ) %>%
set_engine('glmnet')
pen_log_wf <- workflow() %>%
add_recipe(target_encoding_recipe) %>%
add_model(pen_log_mod)
## Grid of values to tune over
tuning_grid <- grid_regular(penalty(),
mixture(),
levels = 5)
## Splits data for CV
folds <- vfold_cv(train, v = 5, repeats = 1)
## Run the CV
cv_results <- pen_log_wf %>%
tune_grid(resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc))
best_tune <- cv_results %>%
select_best('roc_auc')
# Finalize wf and fit it
final_wf <-
pen_log_wf %>%
finalize_workflow(best_tune) %>%
fit(data = train)
# Predict
final_wf %>%
predict(new_data = test, type = 'prob')
# Finalize wf and fit it
final_wf <-
pen_log_wf %>%
finalize_workflow(best_tune) %>%
fit(data = train)
# Finalize wf and fit it
final_wf <-
pen_log_wf %>%
finalize_workflow(best_tune) %>%
fit(data = train)
# Predict
final_wf %>%
predict(new_data = test, type = 'prob')
# Predict
pen_log_preds <- final_wf %>%
predict(new_data = test, type = 'prob')
# Predict
pen_log_preds <- final_wf %>%
predict(new_data = test, type = 'prob')
pen_log_submission <- pen_log_preds %>%
mutate(Id = row_number()) %>%
rename("Action" = ".pred_1") %>%
select(3,2)
vroom_write(x=pen_log_submission, file="./pen_log_reg.csv", delim=",")
best_tune
